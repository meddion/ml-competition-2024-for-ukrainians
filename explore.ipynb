{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id\n",
    "\n",
    "- **Description**: A unique identifier for each row.\n",
    "- **Relevance**: Not useful for prediction. It serves only as an identifier.\n",
    "\n",
    "Item_Identifier\n",
    "\n",
    "- **Description**: A unique identifier for each product.\n",
    "- **Relevance**: Generally not directly useful for prediction. However, it can be used for aggregating statistics like mean sales per item.\n",
    "\n",
    "Item_Weight\n",
    "\n",
    "- **Description**: The weight of the product.\n",
    "- **Relevance**: Potentially useful for prediction as it can impact transportation costs and consumer preference.\n",
    "\n",
    "Item_Fat_Content\n",
    "\n",
    "- **Description**: Describes whether the product is low fat or regular.\n",
    "- **Relevance**: Useful for prediction as it can influence consumer choices and sales.\n",
    "\n",
    "Item_Visibility\n",
    "\n",
    "- **Description**: The percentage of total display area of all products in a store allocated to this particular product.\n",
    "- **Relevance**: Highly relevant for prediction. Higher visibility can lead to higher sales.\n",
    "\n",
    "Item_Type\n",
    "\n",
    "- **Description**: The category to which the product belongs.\n",
    "- **Relevance**: Highly relevant for prediction as different categories may have different sales patterns.\n",
    "\n",
    "Item_MRP\n",
    "\n",
    "- **Description**: Maximum Retail Price (list price) of the product.\n",
    "- **Relevance**: Highly relevant for prediction as price is a major factor influencing sales.\n",
    "\n",
    "Outlet_Identifier\n",
    "\n",
    "- **Description**: A unique identifier for the store.\n",
    "- **Relevance**: Useful for prediction to capture store-specific effects, such as location and size.\n",
    "\n",
    "Outlet_Establishment_Year\n",
    "\n",
    "- **Description**: The year the store was established.\n",
    "- **Relevance**: Potentially relevant for prediction. Older stores might have more established customer bases or different sales trends.\n",
    "\n",
    "Outlet_Size\n",
    "\n",
    "- **Description**: The size of the store in terms of square footage.\n",
    "- **Relevance**: Highly relevant for prediction as store size can influence the number of products displayed and sales.\n",
    "\n",
    "Outlet_Location_Type\n",
    "\n",
    "- **Description**: The type of city in which the store is located (e.g., Tier 1, Tier 2).\n",
    "- **Relevance**: Highly relevant for prediction. Location type can impact foot traffic and sales.\n",
    "\n",
    "Outlet_Type\n",
    "\n",
    "- **Description**: The type of store (e.g., Grocery Store, Supermarket Type1).\n",
    "- **Relevance**: Highly relevant for prediction as different types of stores may have different customer bases and sales patterns.\n",
    "\n",
    "Item_Outlet_Sales\n",
    "\n",
    "- **Description**: The sales of the product in a particular store. This is the target variable.\n",
    "- **Relevance**: This is the target variable you are trying to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    Normalizer,\n",
    ")\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FatContentMapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.mapping = {\"low fat\": \"Low Fat\", \"LF\": \"Low Fat\", \"reg\": \"Regular\"}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"Item_Fat_Content\"] = X[\"Item_Fat_Content\"].replace(self.mapping)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class ItemIdentifierFrequencyMapper(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.item_identifier_counts = X[\"Item_Identifier\"].value_counts()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"Item_Identifier\"] = X[\"Item_Identifier\"].map(self.item_identifier_counts)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class Log1pTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.log1p(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return np.expm1(X)\n",
    "\n",
    "\n",
    "def perturb_data(df, n_samples=1000, noise_level=0.05):\n",
    "    \"\"\"\n",
    "    Generate new samples by adding Gaussian noise to the original data.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The original high-sales data.\n",
    "    n_samples (int): Number of new samples to generate.\n",
    "    noise_level (float): The standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with the new samples.\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for _ in range(n_samples):\n",
    "        # Select a random row\n",
    "        random_row = df.sample(n=1).iloc[0]\n",
    "        perturbed_row = random_row.copy()\n",
    "\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            perturbed_row[col] += np.random.normal(0, noise_level * df[col].std())\n",
    "        new_data.append(perturbed_row)\n",
    "\n",
    "    return pd.DataFrame(new_data)\n",
    "\n",
    "\n",
    "def augment_data(df, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Augment data by creating new samples with slight modifications.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The original high-sales data.\n",
    "    n_samples (int): Number of new samples to generate.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with the augmented samples.\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for _ in range(n_samples):\n",
    "        random_row = df.sample(n=1).iloc[0]\n",
    "        augmented_row = random_row.copy()\n",
    "\n",
    "        # Slightly modify numerical features\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            augmented_row[col] *= np.random.uniform(0.9, 1.1)\n",
    "        new_data.append(augmented_row)\n",
    "\n",
    "    return pd.DataFrame(new_data)\n",
    "\n",
    "\n",
    "def combine_with_synth_data(og_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # n_samples = np.sum(og_sales < og_sales.quantile(0.90)) // 2\n",
    "    combined_synth_df = og_df.copy()\n",
    "    og_sales = og_df[\"Item_Outlet_Sales\"]\n",
    "    idxs_with_coefs = [\n",
    "        (og_sales > og_sales.quantile(0.90), 0.5),\n",
    "        (og_sales > og_sales.quantile(0.99), 2.0),\n",
    "        (og_sales > og_sales.quantile(0.999), 4.0),\n",
    "    ]\n",
    "    for idx, coef in idxs_with_coefs:\n",
    "        selected_rows = og_df[idx]\n",
    "        n_samples = int(coef * len(selected_rows))\n",
    "        print(\"Number of selected rows to generate new data from: \", len(selected_rows))\n",
    "        print(\"Number of samples to generate per synth: \", n_samples)\n",
    "        perturb_df = perturb_data(selected_rows, n_samples=n_samples, noise_level=0.05)\n",
    "        augmented_df = augment_data(selected_rows, n_samples=n_samples)\n",
    "        combined_synth_df = pd.concat(\n",
    "            [combined_synth_df, perturb_df, augmented_df], ignore_index=True\n",
    "        )\n",
    "\n",
    "\n",
    "def drop_n_largest(df, column, n):\n",
    "    largest_indices = df.nlargest(n, column).index\n",
    "    df_dropped = df.drop(largest_indices)\n",
    "\n",
    "    return df_dropped\n",
    "\n",
    "\n",
    "def check_consistency(df):\n",
    "    # Check for inconsistencies in Outlet information\n",
    "    outlet_columns = [\n",
    "        \"Outlet_Establishment_Year\",\n",
    "        \"Outlet_Size\",\n",
    "        \"Outlet_Location_Type\",\n",
    "        \"Outlet_Type\",\n",
    "    ]\n",
    "    outlet_inconsistencies = df.groupby(\"Outlet_Identifier\")[outlet_columns].nunique()\n",
    "\n",
    "    item_columns = [\"Item_Weight\", \"Item_Fat_Content\", \"Item_Type\"]\n",
    "    item_inconsistencies = df.groupby(\"Item_Identifier\")[item_columns].nunique()\n",
    "\n",
    "    return outlet_inconsistencies, item_inconsistencies\n",
    "\n",
    "\n",
    "def print_category_info(df: pd.DataFrame, categorical_cols: List[str]):\n",
    "    print(f\"# of categories: {len(categorical_cols)}\")\n",
    "    for coll_name in categorical_cols:\n",
    "        values = df[coll_name].unique()\n",
    "        print(f\"{coll_name} {values} {len(values)}\")\n",
    "\n",
    "    ohencoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    ohencoder.fit_transform(df[categorical_cols]).shape\n",
    "\n",
    "\n",
    "def category_bar_plot(category_counts):\n",
    "    # Plotting with seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=category_counts.index, y=category_counts.values, palette=\"viridis\")\n",
    "    plt.title(\"Category Occurrences\")\n",
    "    plt.xlabel(\"Item Type\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10, fontweight=\"bold\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    return np.sqrt(np.mean((np.log1p(y_pred) - np.log1p(y_true)) ** 2))\n",
    "\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "\n",
    "\n",
    "def plot_predictions(\n",
    "    model: RegressorMixin,\n",
    "    X_train: np.array,\n",
    "    y_train: np.array,\n",
    "    X_val: np.array,\n",
    "    y_val: np.array,\n",
    "    output_transform: Callable | None = None,\n",
    ") -> Tuple[pd.array, float]:\n",
    "    def predict(X, y):\n",
    "        preds = model.predict(X)\n",
    "        if output_transform is not None:\n",
    "            preds = output_transform(preds)\n",
    "            y = output_transform(y)\n",
    "\n",
    "        score = rmsle(y, preds)\n",
    "\n",
    "        return preds, y, score\n",
    "\n",
    "    preds_train, y_train, score_train = predict(X_train, y_train)\n",
    "    preds_val, y_val, score_val = predict(X_val, y_val)\n",
    "    print(\"Train score\", score_train)\n",
    "    print(\"Evaluation score\", score_val)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    def plot_stairs(axes, pred, y, ds_type):\n",
    "        t_values, t_bins = np.histogram(y)\n",
    "        axes[0].stairs(t_values, t_bins, fill=True, color=\"blue\")\n",
    "        axes[0].set_title(f\"{ds_type} Target values\")\n",
    "\n",
    "        values, bins = np.histogram(pred)\n",
    "        axes[1].stairs(values, bins, fill=True, color=\"red\")\n",
    "        axes[1].set_title(\"Predicted values\")\n",
    "\n",
    "        axes[2].plot(range(len(y)), y, c=\"blue\", label=\"Target\")\n",
    "        axes[2].plot(range(len(pred)), pred, c=\"red\", label=\"Predicted\")\n",
    "        axes[2].set_title(\"Target vs Predicted\")\n",
    "\n",
    "    plot_stairs(axes[0], preds_train, y_train, f\"[Train {score_train:.3f}]\")\n",
    "    plot_stairs(axes[1], preds_val, y_val, f\"[Validation {score_val:.3f}]\")\n",
    "\n",
    "    return preds_val, score_val\n",
    "\n",
    "\n",
    "PREDICTIONS_DIR = Path(\"predictions\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model: RegressorMixin,\n",
    "    test: pd.DataFrame,\n",
    "    file_name: str | None,\n",
    "    output_transform: Callable | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    test_pred = model.predict(test)\n",
    "    if output_transform is not None:\n",
    "        test_pred = output_transform(test_pred)\n",
    "\n",
    "    test_result = pd.DataFrame(\n",
    "        {\"id\": test[\"id\"].values, \"Item_Outlet_Sales\": test_pred.astype(np.int32)},\n",
    "        index=None,\n",
    "    )\n",
    "\n",
    "    if file_name is not None:\n",
    "        joblib_file = MODELS_DIR / f\"{file_name}.pkl\"\n",
    "        joblib.dump(model, joblib_file)\n",
    "\n",
    "        pred_file = PREDICTIONS_DIR / f\"{file_name}.csv\"\n",
    "        test_result.to_csv(pred_file, index=False)\n",
    "\n",
    "    return test_result\n",
    "\n",
    "\n",
    "def compare_distributions(og_df, gen_df):\n",
    "    # Summary stats\n",
    "    print(og_df.describe())\n",
    "    print(gen_df.describe())\n",
    "\n",
    "    # Check for missing values\n",
    "    print(og_df.isnull().sum())\n",
    "    print(gen_df.isnull().sum())\n",
    "\n",
    "    for column in og_df.columns:\n",
    "        if column in gen_df.columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.histplot(og_df[column], color=\"blue\", label=\"Original\", kde=True)\n",
    "            sns.histplot(gen_df[column], color=\"red\", label=\"Generated\", kde=True)\n",
    "            plt.title(f\"Distribution of {column}\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"datasets/train.csv\")\n",
    "test_df = pd.read_csv(\"datasets/test.csv\")\n",
    "og_df = pd.read_csv(\"datasets/Big Sales Data.csv\")\n",
    "\n",
    "train_df = train_df.drop(\"id\", axis=1)\n",
    "features = train_df.drop([\"Item_Outlet_Sales\"], axis=1)\n",
    "log1p_transformer = Log1pTransformer()\n",
    "target = log1p_transformer.transform(train_df[\"Item_Outlet_Sales\"])\n",
    "\n",
    "numeric_cols = [\n",
    "    \"Item_Weight\",\n",
    "    \"Item_Visibility\",\n",
    "    \"Item_MRP\",\n",
    "    \"Outlet_Establishment_Year\",\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Item_Identifier\",\n",
    "    \"Item_Fat_Content\",\n",
    "    \"Item_Type\",\n",
    "    \"Outlet_Identifier\",\n",
    "    \"Outlet_Size\",\n",
    "    \"Outlet_Location_Type\",\n",
    "    \"Outlet_Type\",\n",
    "]\n",
    "\n",
    "\n",
    "def new_pipeline(\n",
    "    model, scaler=StandardScaler(), one_hot=True, item_ident_freq_mapper=True\n",
    ") -> Pipeline:\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", scaler),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = \"passthrough\"\n",
    "    if one_hot:\n",
    "        categorical_transformer = Pipeline(\n",
    "            steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    "        )\n",
    "\n",
    "    preprocessor = Pipeline(steps=[])\n",
    "    preprocessor.steps.append((\"fat_content_mapper\", FatContentMapper()))\n",
    "\n",
    "    if item_ident_freq_mapper and \"Item_Identifier\" in categorical_cols:\n",
    "        preprocessor.steps.append(\n",
    "            (\"item_identifier_frequency_mapper\", ItemIdentifierFrequencyMapper())\n",
    "        )\n",
    "\n",
    "    preprocessor.steps.append(\n",
    "        (\n",
    "            \"column_transformer\",\n",
    "            ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", numeric_transformer, numeric_cols),\n",
    "                    (\"cat\", categorical_transformer, categorical_cols),\n",
    "                ],\n",
    "                remainder=\"drop\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    model: RegressorMixin,\n",
    "    features: pd.DataFrame,\n",
    "    target: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    param_grid: Dict[str, Any] | None = None,\n",
    "    out_file_name: str | None = None,\n",
    "    cv=4,\n",
    ") -> RegressorMixin:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        features, target, test_size=0.2, random_state=99\n",
    "    )\n",
    "\n",
    "    if param_grid is None:\n",
    "        best_model = model.fit(X_train, y_train)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid=param_grid,\n",
    "            scoring=rmsle_scorer,\n",
    "            cv=cv,\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "        print(\"Best Model:\", grid_search.best_estimator_)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "    plot_predictions(\n",
    "        best_model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        output_transform=log1p_transformer.inverse_transform,\n",
    "    )\n",
    "\n",
    "    predict(\n",
    "        best_model,\n",
    "        test,\n",
    "        out_file_name,\n",
    "        output_transform=log1p_transformer.inverse_transform,\n",
    "    )\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"model__num_leaves\": [31, 50, 70],\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"model__n_estimators\": [50, 100, 200, 250],\n",
    "}\n",
    "\n",
    "lgbm_model = LGBMRegressor()\n",
    "lgbm_pipeline = new_pipeline(lgbm_model, one_hot=True, item_ident_freq_mapper=True)\n",
    "\n",
    "best_lgbm_model = train_and_evaluate(\n",
    "    lgbm_pipeline,\n",
    "    features,\n",
    "    target,\n",
    "    test_df,\n",
    "    param_grid,\n",
    "    out_file_name=\"lgbm_v2_output\",\n",
    "    cv=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyperparameters:\n",
    "# {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.1, 'model__max_depth': 7,\n",
    "# 'model__n_estimators': 200, 'model__subsample': 1.0}\n",
    "\n",
    "param_grid = {\n",
    "    \"model__max_depth\": [3, 5, 7],\n",
    "    \"model__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"model__n_estimators\": [50, 100, 200],\n",
    "    \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"model__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_pipeline = new_pipeline(lgbm_model, one_hot=True, item_ident_freq_mapper=True)\n",
    "\n",
    "best_xgb_model = train_and_evaluate(\n",
    "    xgb_pipeline,\n",
    "    features,\n",
    "    target,\n",
    "    test_df,\n",
    "    param_grid,\n",
    "    out_file_name=\"xgb_v1_output\",\n",
    "    cv=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": [0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "    \"solver\": [\n",
    "        # \"auto\",\n",
    "        # \"svd\",\n",
    "        # \"cholesky\",\n",
    "        # \"lsqr\",\n",
    "        # \"sparse_cg\",\n",
    "        \"sag\",\n",
    "        # \"saga\",\n",
    "    ],  # Solvers\n",
    "}\n",
    "\n",
    "\n",
    "lr_model = Ridge()\n",
    "lr_pipeline = new_pipeline(\n",
    "    lr_model, scaler=MinMaxScaler(), one_hot=True, item_ident_freq_mapper=True\n",
    ")\n",
    "\n",
    "\n",
    "best_xgb_model = train_and_evaluate(\n",
    "    lr_model,\n",
    "    features,\n",
    "    target,\n",
    "    test_df,\n",
    "    param_grid,\n",
    "    out_file_name=\"lr_v1_output\",\n",
    "    cv=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"model__depth\": [7, 9, 12],\n",
    "#     \"model__learning_rate\": [0.2],\n",
    "#     \"model__iterations\": [\n",
    "#         1000,\n",
    "#         2000,\n",
    "#         5000,\n",
    "#     ],  # CatBoost parameter for n_estimators\n",
    "#     \"model__subsample\": [0.8, 1.0],\n",
    "#     \"model__colsample_bylevel\": [\n",
    "#         0.8,\n",
    "#         1.0,\n",
    "#     ],\n",
    "# }\n",
    "# best_cat_boost_params = {}\n",
    "\n",
    "param_grid = None\n",
    "best_cat_boost_params = dict(\n",
    "    colsample_bylevel=1.0, depth=7, iterations=1000, learning_rate=0.2, subsample=1.0\n",
    ")\n",
    "\n",
    "cat_model = CatBoostRegressor(**best_cat_boost_params, verbose=True)\n",
    "cat_pipeline = new_pipeline(cat_model, one_hot=True, item_ident_freq_mapper=True)\n",
    "\n",
    "best_cat_model = train_and_evaluate(\n",
    "    cat_pipeline,\n",
    "    features,\n",
    "    target,\n",
    "    test_df,\n",
    "    param_grid,\n",
    "    out_file_name=\"cat_v8_output\",\n",
    "    cv=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = None\n",
    "best_cat_boost_params = dict(\n",
    "    colsample_bylevel=1.0, depth=7, iterations=1000, learning_rate=0.2, subsample=1.0\n",
    ")\n",
    "\n",
    "model = CatBoostRegressor(**best_cat_boost_params, verbose=True)\n",
    "cat_pipeline = new_pipeline(\n",
    "    cat_model, scaler=MinMaxScaler(), one_hot=True, item_ident_freq_mapper=True\n",
    ")\n",
    "\n",
    "best_cat_model = train_and_evaluate(\n",
    "    cat_pipeline,\n",
    "    features,\n",
    "    target,\n",
    "    test_df,\n",
    "    param_grid,\n",
    "    out_file_name=\"cat_v9_output\",\n",
    "    cv=4,\n",
    ")\n",
    "\n",
    "# Prev:\n",
    "# Train score 0.6746445262930283\n",
    "# Evaluation score 0.7104292188009841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "p = joblib.load(MODELS_DIR / \"cat_v8_output.pkl\")\n",
    "\n",
    "importances = p[\"model\"].feature_importances_\n",
    "categorical_cols_hot_enc = (\n",
    "    p[\"preprocessor\"][\"column_transformer\"][\"cat\"]\n",
    "    .get_feature_names_out(categorical_cols)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "\n",
    "cat_lengths = defaultdict(lambda: 0)\n",
    "for cat in categorical_cols:\n",
    "    for cat_hot_enc in categorical_cols_hot_enc:\n",
    "        if cat_hot_enc.startswith(cat):\n",
    "            cat_lengths[cat] += 1\n",
    "\n",
    "prev = len(numeric_cols)\n",
    "new_importances = importances.tolist()[:prev]\n",
    "for cat in categorical_cols:\n",
    "    cat_len = cat_lengths[cat]\n",
    "    new_importances.append(sum(importances[prev : prev + cat_len]))\n",
    "    prev += cat_len\n",
    "\n",
    "\n",
    "feature_names = numeric_cols + categorical_cols\n",
    "\n",
    "feature_importances = pd.DataFrame(\n",
    "    {\"Feature\": feature_names, \"Importance\": new_importances}\n",
    ")\n",
    "feature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importances[\"Feature\"], feature_importances[\"Importance\"])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance from LGBM Model\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
